{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OptML_project_main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjCb8fi2Aied"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcD5uHL-Ad7O"
      },
      "source": [
        "import torch\n",
        "from torch import  nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import pandas as pd\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zplslvOhAmLh"
      },
      "source": [
        "## Set path to drive to load/save data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHpmH1sKAtNd"
      },
      "source": [
        "use_drive=True\n",
        "if use_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "# set the path to load/save models\n",
        "path_to_files=\"drive/MyDrive/OptML_project/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z07IQTVB7AM"
      },
      "source": [
        "'''\n",
        "Generate MNIST train and test data.\n",
        "RETURN:\n",
        "    - trainset: 60000 train samples\n",
        "    - testset: 10000 test samples\n",
        "'''\n",
        "def generate_data():\n",
        "    transform = transforms.ToTensor()\n",
        "\n",
        "    # Load and transform data\n",
        "    trainset = datasets.MNIST('data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    testset = datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    return trainset, testset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGCWhQo4Khyi"
      },
      "source": [
        "class LeNet5(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        #First convolution (LeNet5 us 32x32 image as input hence the padding of 2)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2, bias=True)\n",
        "        #First Max-Pooling\n",
        "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2)\n",
        "        #Second convolution\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        #Second Max-Pooling\n",
        "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        #Fully Connected Layers\n",
        "        self.fc1 = torch.nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = torch.nn.Linear(120, 84)\n",
        "        self.fc3 = torch.nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Do the convolution and then perform ReLu\n",
        "        x = torch.nn.functional.relu(self.conv1(x))\n",
        "        # Max-Pooling with 2x2 grid\n",
        "        x = self.max_pool_1(x)\n",
        "        # Do the second convolution and then ReLu\n",
        "        x = torch.nn.functional.relu(self.conv2(x))\n",
        "        # Max-Pooling with 2x2 grid\n",
        "        x = self.max_pool_2(x)\n",
        "        # Flatten to have 16*5*5 columns, FC1 and then ReLu\n",
        "        x = torch.nn.functional.relu(self.fc1(x.view(-1, 16*5*5)))\n",
        "        # FC2 then ReLu\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        #FC3\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdZ7zG-gO5cc"
      },
      "source": [
        "optimizer_methods = {'SGD': (lambda parameters, eta, momentum: optim.SGD(parameters(), eta, momentum = momentum))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbfz3PBqhmHp"
      },
      "source": [
        "'''\n",
        "The train_model method will train the model according to the given parameters and it\n",
        "will store the results after each epoch, like the train/test accuracy and train/test loss\n",
        "\n",
        "PARAMETERS:\n",
        "    - model: nn model\n",
        "    - trainset: training set\n",
        "    - testset: testing set\n",
        "    - batch_size: size of the batch used during training\n",
        "    - eta: learning rate\n",
        "    - criterion: nn loss\n",
        "    - nb_epochs: number of epochs used during training\n",
        "    - momentum: value of the momentum used\n",
        "    - optimizer_name: string corresponding to the optimizer name\n",
        "    - device: torch.device \"cuda\" if using GPU or \"cpu\" when not\n",
        "    - results: empty results list\n",
        "    - warmup : when set to True will perform the gradual warmup\n",
        "    \n",
        "'''\n",
        "def train_model(model, trainset, testset, batch_size, eta, criterion,\n",
        "                nb_epochs, momentum, optimizer_name, device, results,warmup=False):\n",
        "\n",
        "    NB_TRAIN_SAMPLES = len(trainset)\n",
        "    NB_TEST_SAMPLES = len(testset)\n",
        "    TOT_TIME = 0\n",
        "\n",
        "\n",
        "    # default pytorch functions which are useful for loading testing and training data\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=NB_TEST_SAMPLES)\n",
        "\n",
        "    optimizer = optimizer_methods[optimizer_name](model.parameters, eta, momentum)\n",
        "\n",
        "    for epoch in range(nb_epochs):\n",
        "        if epoch%5==0:\n",
        "          print('Epoch :',epoch)\n",
        "        curr_train_acc = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        if (epoch<9 and warmup):\n",
        "          print(epoch)\n",
        "          k=10\n",
        "          ep=epoch+1\n",
        "          u_eta = ((k+1-ep)/k)*eta + ((ep-1)*batch_size*eta)/(k*64.0)\n",
        "          optimizer = optimizer_methods[optimizer_name](model.parameters, u_eta, momentum)\n",
        "\n",
        "\n",
        "        if (epoch == 9) and (warmup):\n",
        "            print(epoch)\n",
        "            u_eta = eta*(batch_size/64.0)\n",
        "            optimizer = optimizer_methods[optimizer_name](model.parameters, u_eta, momentum)\n",
        "        \n",
        "        \n",
        "\n",
        "        for minibatch, label in train_loader:\n",
        "            minibatch, label = minibatch.to(device), label.to(device)\n",
        "            output = model.forward(minibatch)\n",
        "            _, pred_class = output.max(1)\n",
        "            loss = criterion(output, label)\n",
        "            # Backward and optimize\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            curr_train_acc += (label == pred_class).sum().item()\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        TOT_TIME += duration\n",
        "        train_loss = loss.item()\n",
        "        # Testing part\n",
        "        with torch.no_grad():\n",
        "            testset, test_labels = next(iter(test_loader))  # load the one and only test batch\n",
        "            testset, test_labels = testset.to(device), test_labels.to(device)\n",
        "\n",
        "            output = model.forward(testset)\n",
        "            loss = criterion(output, test_labels)\n",
        "\n",
        "            test_loss = loss.item()\n",
        "            _, pred_class = output.max(1)\n",
        "\n",
        "            curr_test_acc = (test_labels == pred_class).sum().item()\n",
        "\n",
        "        # Append the epoch data to results\n",
        "        results.append({'model': 'LeNet5',\n",
        "                        'train_accuracy': curr_train_acc/NB_TRAIN_SAMPLES, 'test_accuracy': curr_test_acc/NB_TEST_SAMPLES,\n",
        "                        'train_loss': train_loss, 'test_loss': test_loss, 'epoch': epoch + 1,\n",
        "                        'time': duration, 'batch_size': batch_size,\n",
        "                        'lr': eta,'momentum':momentum})\n",
        "\n",
        "    print(f'MODEL: {\"LeNet5\"}, BATCH_SIZE: {batch_size}, TIME : {TOT_TIME}, CRITERION: {str(criterion)}, EPOCHS: {nb_epochs}, train_accuracy:{curr_train_acc/NB_TRAIN_SAMPLES}, test_accuracy: {curr_test_acc/NB_TEST_SAMPLES}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK7XGDJ9PY3w"
      },
      "source": [
        "def sqrtScaleRule(n0,b0,batchlist):\n",
        "  bs_lr=[]\n",
        "  for b in batchlist:\n",
        "    lr = n0*(np.sqrt(b/float(b0)))\n",
        "    bs_lr.append((b,lr))\n",
        "  return bs_lr\n",
        "\n",
        "def linearScaleRule(n0,b0,batchlist):\n",
        "  bs_lr=[]\n",
        "  for b in batchlist:\n",
        "    lr = n0*(b/float(b0))\n",
        "    bs_lr.append((b,lr))\n",
        "  return bs_lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQn9Z9cI0I5y"
      },
      "source": [
        "def main():\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    print(device)\n",
        "    print('Training model on different batch size values ...')\n",
        "\n",
        "    # Effect of batch size\n",
        "    batch_size_values = [64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
        "    eta = 0.01\n",
        "    epochs=30\n",
        "    model = LeNet5()\n",
        "    results = []\n",
        "    for batch in batch_size_values:\n",
        "        trainset, testset = generate_data()\n",
        "        model_cloned=copy.deepcopy(model)\n",
        "        train_model(model_cloned.to(device), trainset, testset, batch, eta,\n",
        "                    nn.CrossEntropyLoss(), epochs, 0, 'SGD', device, results,warmup=False)\n",
        "    \n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(path_to_files+'BaseLeNet5.csv')\n",
        "\n",
        "    ''' Uncomment this to run our model with the sqrt scaling rule\n",
        "    eta = 0.01\n",
        "    bs_lr = sqrtScaleRule(eta,64,batch_size_values)\n",
        "\n",
        "    epochs=30\n",
        "    model = LeNet5()\n",
        "    results = []\n",
        "    for bl in b:\n",
        "        trainset, testset = generate_data()\n",
        "        model_cloned=copy.deepcopy(model)\n",
        "        train_model(model_cloned.to(device), trainset, testset, bl[0], bl[1],\n",
        "                    nn.CrossEntropyLoss(), epochs, 0, 'SGD', device, results,warmup=False)\n",
        "    \n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(path_to_files+'sqrtLrLeNet5.csv')\n",
        "    '''\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJjLqwDz2l8c"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSoM3m2R-Ak_"
      },
      "source": [
        "df_base = pd.read_csv(path_to_files+'BaseLeNet5.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZOqfLHpkXL0"
      },
      "source": [
        "def plot(results):\n",
        "  colors = {64: 'tab:blue', 128: 'tab:orange', 256: 'tab:green', 512: 'tab:red', 1024: 'tab:purple', 2048 : 'tab:brown', 4096: 'tab:pink', 8192: 'tab:gray'}\n",
        "  fig, axs = plt.subplots(1, 2)\n",
        "  fig.tight_layout()\n",
        "  results = results.set_index('epoch')\n",
        "\n",
        "  group_batch = results.groupby(['batch_size'])\n",
        "  for batch, df in group_batch:\n",
        "\n",
        "        df['train_accuracy'].plot(ax = axs[0], color=colors[batch])\n",
        "        axs[0].set_title('Train accuracy')\n",
        "        axs[0].set(ylabel='Accuracy')\n",
        "\n",
        "        df['test_accuracy'].plot(ax = axs[1], color=colors[batch], label=str(df['batch_size'].unique()[0]))\n",
        "        axs[1].set_title('Test accuracy')\n",
        "        axs[1].set(ylabel='Accuracy')\n",
        "        axs[1].yaxis.set_label_text(\"\")\n",
        "        axs[1].legend(title ='Batch Size',bbox_to_anchor=(1.05, 1),loc='upper left')\n",
        "        '''\n",
        "        df['train_loss'].plot(ax = axs[1, 0], color=colors[batch])\n",
        "        axs[1, 0].set_title('Train loss')\n",
        "        axs[1, 0].set(xlabel='Epoch', ylabel='Loss')\n",
        "\n",
        "        df['test_loss'].plot(ax = axs[1, 1], color=colors[batch])\n",
        "        axs[1, 1].set_title('Test loss')\n",
        "        axs[1, 1].set(xlabel='Epoch', ylabel='Loss')\n",
        "        axs[1, 1].yaxis.set_label_text(\"\")\n",
        "        '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1OSVK5iYRgh"
      },
      "source": [
        "plot(df_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocQom6N_LxRG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}